###### tags: `李宏毅機器學習`
# 李宏毅機器學習第1,2集-預測本頻道觀看人數 - 機器學習基本概念簡介

# 第一集

# 1. 機器學習的不同類型

機器學習就是讓機器具備找一個函式的能力


## 1.1 Regression(預測數值)

要找的函式，其輸出是一個數值

## 1.2 Classification(預測為分類的問題)

函式的輸出，就是從設定好的選項裡選擇一個當作輸出

## 1.3 Structured Learning

機器產生有結構的東西的問題，學會創造
>例如:生圖

# 2. Case Study：預測頻道流量

## 2.1 訓練三步驟

### Step 1：定義function

![](https://i.imgur.com/8eUGb1x.png)
>預測下一天頻道觀看次數

- $y$ **是要預測的東西**
- $x_1$ **是頻道前一天總觀看人數，**跟 $y$ 一樣都是數值
- $b$ 跟 $w$ 是**未知的參數**，透過準備資料找出最適合的參數
    
    **猜測函數：**
    
    **未來點閱次數的函式** $f$，是前一天的點閱次數乘上 $w$ 再加上 $b$。猜測往往是對問題本質上的了解，是 **domain knowledge**
    >所以常常說model，在機器學習中就是代表一個帶有未知參數的函式
    

名詞定義：

- Feature：function 中已知的訊息（$x_1$,輸入）
- Weight：未知參數，與 feature 直接相乘(w)
- Bias：未知參數，直接相加(b)

### Step 2：定義loss function

loss 也是一個 function，它的輸入是 model 中的參數（$w,b$），就是帶入不同的函式的意思，去看哪一個函式比較好所需的公式，稱為loss

**物理意義：function 輸出的值代表預測輸出與實際輸出的差距，這筆數值好還是不好**
>所謂的label就是實際輸出的數值y
    
**L 越大，代表一組參數越不好，這個大 L 越小，代表現在這一組參數越好**
    
- **計算方法：求取估測的值跟實際的值（label） 之間的差距**
    - MAE（mean absolute error）>>相減後絕對值全加起來取平均
    - MSE（mean square error）>>相減平方全加取平均
    - Cross-entropy：計算**機率分布**之間的差距>>之後再說



**Error Surface：**

試**不同的參數**，然後**計算 loss 所畫出來的等高線圖**
![](https://i.imgur.com/FqUuEpT.png)


### Step 3：Optimization

找到能讓損失函數值最小的參數(找到最好的函式)

**方法：**

**Gradient Descent（梯度下降）**

![](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/be2fc769-5ce1-4aa3-90d4-05fcd0107cc8/Untitled.png)

**步驟：**

1. 隨機選取初始值 $w_0$
2. 計算 $w=w_0$ 時，$*w$* 對 loss 的微分是多少
3. 根據微分（梯度）的方向，改變參數的值

>如果斜率是負的，則左高右低，則要往右邊跑才能使loss function小 
![](https://i.imgur.com/aDKZEBN.png)




 
    改變的大小取決於：
    
    - 斜率的大小，斜率大，步伐大
    - 學習率的大小**（超參數>自己設定的參數）**
4. 什麽時候停下來？
    1. 自己設置上限(例:自己設更新100萬次就不更新)
    2. 理想情況：微分值為 0（極小值點），不會再更新 ⇒ **有可能陷入局部最小值**，不能找到全局最小值
    **事實上：局部最小值不是真正的問題！！！**

**推廣到多個參數：**

![](https://i.imgur.com/1aG1fPT.png)

![](https://i.imgur.com/dxp3uby.png)


## 2.2 Linear Model
因為觀察到是每七天為一個循環，所以
根據周期性修改模型，考慮**前7天，甚至更多天**的值

![](https://i.imgur.com/xEedGax.png)
>這種feature乘上一個weight再加上一個bias的model稱為linear model
---
# 第2集
### 2.2.1 Model Bias

**問題：**

模型遇到無法模擬或描述真實情況的狀況
例:linear model無法模擬非線性的狀況  

**解決：**

需要一個更複雜的、更有彈性的、有未知參數的 function

## 2.3 Piecewise(分段) Linear Curves（Sigmoid）

### 2.3.1 模型定義
>此難描述較抽象

**定義：**

由多段鋸齒狀的線段所組成的線，可以看作是一個常數(平移)，再加上一堆藍色的 function**（Hard Sigmoid）**

$$
y=b+\sum_isigmoid(b_i+w_ix_i)
$$

![](https://i.imgur.com/Q83hnYP.png)
>例:紅色前面斜坡那段=0線段(平移那個藍線)+1線段


用一條曲線來近似描述這條藍色的曲線：**Sigmoid 函數（S 型的 function）**(因為沒有一個function可以描述一下固定一下斜坡一下固定的，所以用sigmoid近似這種函數)

![](https://i.imgur.com/E3HyTbc.png)


事實上，sigmoid 的個數就是神經網絡中的一層的 neuron 節點數（使用幾個 sigmoid 是**超參數**）

**結論：**

1. 可以用 **Piecewise Linear 的 Curves**，去**逼近任何的連續的曲線**
>Piecewise Linear為一個分好多段的函數

2. 每一個 **Piecewise Linear 的 Curves**，都可以用**一大堆藍色的 Function 加上一個常量組合**起來得到
3. 只要有**足夠的藍色 Function** 把它加起來，就可以變成任何連續的曲線

### 2.3.2 Sigmoid 函數

$$
Sigmoid：y = c\frac{1}{{1+e^{-(b+wx_1)}}}
$$



- $x_1$ 趨近於正無窮大 ⇒ 收斂於高度 $c$
- $x_1$ 負非常大，分母就會非常大 ⇒ $y$ 值趨近於 0

調整 **$w,b,c$ ，可以得到各種不同的 sigmiod 來逼近”藍色function“，通過求和，最終近似各種不同的 continuous function**

![](https://i.imgur.com/jnL5RwS.png)


- 如果改 $w$，就會改變**斜率**，改變斜坡的坡度
- 如果改 $b$，就可以把 sigmoid function 左右移動
- 如果改 $c$，就可以改變它的高度

**總結：**

利用若干個具有不同 $w,b,c$ 的 Sigmoid 函數與一個常數參數的組合，可以模擬任何一個連續的曲線（非線性函數）
![](https://i.imgur.com/kKTdpcI.png)
>此Piecewise Linear 的 Curves為1+2+3(上面圖的sigma)

**擴展到多個特徵(例:每三天一周期，為三個特徵)：**
>把model看為一整個function，而不是單點


![](https://i.imgur.com/X34hqei.png)


$$
y=b+\sum_isigmoid(b_i+\sum_j(w_{ij}x_j))
$$

![](https://i.imgur.com/IvDpsVJ.png)
>w那麼多是多維，也就是每個sigmoid就是一個多為線性的圖，還是線性的
>而x1,x2,x3是輸入，就是前1天,前2天,前3天自己輸入的值，所以每個r的意義為透過設定不同的三個w的值去造出一個sigmoid函數


- $j$ 等於 1 2 3，$x_1$ 代表前一天的觀看人數，$x_2$ 兩天前觀看人數，$x_3$ 三天前的觀看人數
- 每一個 $i$ 就代表了一個藍色的 function，現在每一個藍色的 function 都用一個 sigmoid function 來近似它
>這個結果可能是一個不規則的function，然後透過自己設定要多少個sigmoid來逼近，設越多，越複雜逼近
>![](https://i.imgur.com/3bsRaLf.png)

- $w_{ij}$ 第 $i$ 個 sigmoid 給第 $j$ 個 feature 的權重(不同特徵(前幾天權重))


轉化為矩陣計算 + 激勵函數形式：

![](https://i.imgur.com/d4Zxi5Z.png)


![](https://i.imgur.com/YTa0VNm.png)
>c轉置只是行向量的意思


![](https://i.imgur.com/81o1pxo.png)


![](https://i.imgur.com/yBWdP1H.png)


總之**：**

![](https://i.imgur.com/w6xibpQ.png)


因為未知參數太多了，所以把所有未知參數放在一個名為日的行向量，來計算loss function

### 2.3.3 寫出 Loss 函數

因為所有參數統稱為 $\theta$，loss 表示為 $L(\theta)$

### 2.3.4 優化過程

就是多維的gradient descent而已

（1）選定**初始參數值**（向量） $\theta_0$

（2）對每個參數求**偏微分**

（3）更新參數，直至設定的次數

![](https://i.imgur.com/uKqHUtr.png)
>三角形代表對行向量每個對L做微分


![](https://i.imgur.com/hP0nsjf.png)
>gradient descent的過程

### ▲ Batch training

每次更新參數時，只使用 **1 個 batch** 裡的資料計算 loss，求取梯度，更新參數

> batch 大小也是**超參數**(自己設)
> 

![](https://i.imgur.com/Dj0ylJx.png)


**Update：每次更新一次參數叫做一次 Update**

**Epoch：把所有 batch 都看過一遍叫做一個 Epoch**

## 2.4 ReLU

把兩個 ReLU 疊起來就等於 **hard sigmoid**

$$
y=b+\sum_imax(0, b_i+w_ix_i)
$$

![兩個 ReLU 可以合成一個 hard sigmoid]![](https://i.imgur.com/hyvUydI.png)


兩個 ReLU 可以合成一個 hard sigmoid
(https://s3-us-west-2.amazonaws.com/secure.notion-static.com/cd0587bf-8974-4917-8837-de1bfcdf5310/Untitled.png)

Sigmoid 和 ReLU 都屬於激勵函數**（Activation Function）**

### ▲ 模型變型 ⇒ 多加幾層

(https://i.imgur.com/703KDSN.png)


函數的函數的函數…的函數

# 3. **引入 Deep Learning**

![](https://i.imgur.com/c4PWe2D.png)
>猜測是因為每層用不同的方法能夠更擬和問題，例如第一層用sigmoid，但還不夠準確，第二層用Relu再更接近擬合
>>(用不同的線段去接近的意思)
>>>看之後的影片疑似是更多層的神經網路可以讓model bias更好，可以找到function的範圍更大

**問題：**

為什麽要“深”，而不“胖”？

![](https://i.imgur.com/GIuo9s7.png)


**Overfitting：在訓練資料上有變好，但是在沒看過的資料上沒有變好**

# 4. 以上所講解的神經網路為--全連接神經網路 Fully-Connected Neural Network

![](https://i.imgur.com/sXpLoGI.png)
在宣告時nn是pytorch裡neural network的套件，linear是老師上課所講的全連接神經網路 Fully-Connected Neural Network，此為一個layer

然後前面定義用甚麼神經網路後，在用activation function去逼近

此例子為輸入10維經過linear layer後變成32維，類推

https://www.bilibili.com/video/BV1Wv411h7kN?p=9&vd_source=3632561eb2e585d1b2266f00299185ef
這個影片講得很好
