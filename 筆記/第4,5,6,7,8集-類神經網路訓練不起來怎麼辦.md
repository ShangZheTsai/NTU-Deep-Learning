###### tags: `李宏毅機器學習`
# 李宏毅機器學習第4,5,6,7,8集-類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point)

# 第四集
討論optimization的gradient descent怎麼更好

# 1. Critical Point

Critical Point 是梯度（gradient）為 0 的點

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/a4cc7136-f170-49b1-a830-dbcda777d18f)



如果 loss 沒有辦法再下降，也許是因為卡在 **critical point** ⇒ **local minima** OR **saddle point**

- **local minima：局部極小值**
    
    卡在 local minima，那可能就無路可走
    
- **saddle point：鞍點(右圖)**
    
    卡在 saddle point，旁邊還是有路可以走
>此圖為對左右來說是最低點(最低點gradient為0)，但對前後是最高點(最高點gradient為0)

## 1.1 如何判斷是local minima還是saddle point?
首先要知道loss function長甚麼樣子
無法完整知道整個損失函數的樣子，但是如果給定某一組參數，比如 $θ'$，在 $θ'$ 附近的損失函數是有辦法寫出來的，雖然 $L(θ)$ 完整的樣子寫不出來，但附近的 $L(θ)$ 可近似為（泰勒級數展開）：
>$L(θ)$在L(θ′)附近可以用下面的表示寫出來


![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/dc064c4d-9cf6-43dd-9c61-989b18661629)


- 第一項中，$L(\theta')$，當 $\theta$ 跟 $\theta'$ 很近的时候，$L$ 很靠近
- 第二項中，$g$ 代表梯度，彌補 $L(\theta')$ 與 $L(\theta)$ 之間的差距；$g$ 的第 $i$ 個 component 就是 $θ$ 的第 $i$ 個 component 對 $L$ 的偏微分(綠色線)
- 第三項中，$H$ 表示海森矩陣，是 $L$ 的二階微分，用來補足剩餘的差距(紅色線)

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/15567f93-669d-49df-bc4a-392c05db2df2)


**在 critical point 附近時：第二項為 0(因微分為0>>gradient為0，根據第三項來判斷 → 只需考察 H 的特徵值**


![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/a47c49e2-e25a-4274-9006-1d43983084fa)
>因為$L(θ)$是代表L($\theta'$)附近的點的意思，所以>0時表示L($\theta'$)為Local minima，反之為Local maxima

>有>0或<0都有表示為有時候$L(θ)$較大有時候較小，表示為Saddle point(有些v帶入為>0有些<0，表示是可以轉的，馬鞍那個圖)

所以算出 $v^THv$ 是正、負或有正有負，來判斷是哪種 critical point
**也可只算 $H$ 就可**

- **所有 eigen value 都是正的(鄭定矩陣定義)**，H 是 positive definite（正定矩陣），此時是 **local minima**
- **所有 eigen value 都是負的**，H 是 negative definite，此時是 **local maxima**
- **如果 eigen value 有正有負**，代表是 **saddle point**

**實例：**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/9446deee-351c-4500-aedb-0224a97cc49a)
>自訂Loss function，知道w1和w2帶0的gradient為0(看等高線圖)，之後看w1和w2為0這個點為local 還是saddle point就用H這個矩陣來看

>**如果走到 saddle point，可以利用 $H$ 的特徵向量確定參數的更新方向**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/55c268c9-9b2d-40bb-b5ba-52439c51a1ec)


假設特徵值小於 0，得到對應的特徵向量 $u$，在 $\theta'$ 的位置加上 $u$，沿著 $u$ 的方向做 update 得到 $\theta$，就可以讓 loss 變小

**實例：**

![**實作上很少運用此方法逃離 saddle point**]
![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/63cde28e-3407-4517-9d71-94d8a674876b)





**實作上很少運用此方法逃離 saddle point**

## 1.2 Local Minima 比 Saddle Point 少的多

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/7ed41721-b1d3-4e40-aff5-26f4aa5e5efb)


Loss 在一個**維度很高的空間中**，**往往只會遇到鞍點而幾乎不會遇到局部極小值點**
從上圖可以看出，正特徵值的數目最多只佔所有特徵值的 60%，這就說明剩餘 40%-50% 的維度都仍然“有路可走”（是 **saddle point**）

# 第五集

# 2. Batch and Momentum
講述為甚麼助教的程式要用batch分批處理要計算gradient的資料

## 2.1 Review：[Optimization with Batch](https://www.notion.so/01-b6979cc5ba9b4e2887d2b5e86c174897)

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/27532417-fde2-47dd-ab55-1d155c39a4b7)


- 在 update 參數的時候，我們是拿 **B 個樣本**出來，算 loss、算 gradient
- **所有的 batch 看過一遍，叫做一個 epoch**

**Shuffle：**

每個 Epoch 開始前往往會重新分一次 Batch，每一個 Epoch 的 Batch 都不一樣
>每次執行Epoch資料會洗牌，不重複

## 2.2 Small Batch v.s. Large Batch
沒用Batch和有用Batch分析

### 2.2.1 基本現象

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/e40abb24-e024-4494-b230-3680421df050)
>一次更新一個樣本數據就是更新一個未知參數，就是一次改變一個未知參數去看他的loss,如一次改變全部未知參數就是全部參數一起改變計算loss

>例如2個參數，在loss圖上，一次改變一個就是一次改變x或y值，全部一起改變就是同時移動x和y值，可斜角移動loss值

- 左邊的 Case 必須要把所有 20 筆 examples 都看完以後，參數才能夠 update 一次
    - 左邊蓄力時間長，但是威力比較大
- 右邊的 Case 只需要 1 筆樣本數據就 update 一次參數。用 1 筆資料算出來的 loss 顯然是比較 **noisy**(指gradient可以往各地方的幅度比較大，所以 update 的方向曲曲折折
    - 右邊技能**冷卻時間短**
    - **但noisy 的 gradient，反而可以幫助訓練**

        
![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/6f388b87-bf13-44eb-98db-d59a55fea604)
>Batch size大反而讓正確率下降

        

### 2.2.2 時間性能

考慮**並行運算**，左邊（大的 batch size）並不一定時間比較長(指的是處理一個batch)

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/8f31a6ed-7676-48bf-a20f-389b564a8549)
>在處理一個Batch中的資料的平行運算

>到1000以上之後平行運算性能到極限，之後時間就上升了

**現象：**

- batch size 是從 1 到 1000 所需要的時間幾乎是一樣的
- 增加到 10000 乃至增加到 60000 的時候，一個 batch 所要耗費的時間確實有隨著 batch size 的增加而逐漸增長，但影響不大

**原因：**

- 有 GPU，可以做並行運算，所以 1000 筆資料所花的時間，**並不是 1 筆資料的 1000 倍**
- GPU 平行運算的能力還是有極限，當 batch size 非常非常巨大的時，**GPU 在跑完一個 Batch 並計算出 Gradient 所花費的時間，還是會隨著 batch size 的增加而逐漸增長**

**對總時間（1 epoch）的影響：**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/92b887c6-2133-4a88-9c0a-187f31a8d57f)
>左是各batch size(x軸)每次更新的時間
>右是一個epoch計算的時間

>此可以看到在1000時時間會最快，所以要看GPU可以平行運算的極限取Batch size讓速度最快



### 結論 1：使用較小的 Batch Size，在更新參數時會有 Noisy ⇒ 有利於訓練

#### 不同的 batch 所用的 loss function 略有差異，可以避免局部極小值“卡住”
![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/da04b7a2-e197-4413-8e84-1f8896cfafd7)
>右是batch size為1，每次更新不同參數的loss function不一樣(實際上的loss 的方程式是一樣的格式，但因參數調整使得這個線條不一樣，但整體差不多(例如都是線性)

>左邊的只有一個loss function卡住就停了，右邊有各種再細調


### 結論 2：使用較小的 Batch Size 可以避免 Overfitting（有利於測試）

> 根據 [*On Large-Batch Training For Deep Learning: Generalization Gap And Sharp Minima*](https://arxiv.org/abs/1609.04836) 這篇 paper 的實驗結果：
> 
> 
> ![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/2638d32e-0688-496b-9630-3e4953e7ea2c)

> 
> 可觀察出 training 的時候都很好，**testing 時大的 batch 較小的 batch 結果差，代表 overfitting**
> 

**一種解釋（尚待可以研究）：**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/cfb33f06-bc6d-4991-8eeb-c77ecaf9a349)


在一個峽谷裡面的 local minima 是壞的 minima；若在一個平原上的 local minima 是好的 minima

假設 testing 的 loss function 是把 training 的 loss function 往右平移
⇒ 對於在一個盆地裡面的 minima 來說，它的在 training 跟 testing 上面的結果不會差太多；但對於在峽谷裡面的 minima 來說，一差就可以天差地遠

**大的 batch size，會傾向於走到峽谷裡面；而小的 batch size，傾向於走到盆地裡面**

- 小的 batch 有很多的 loss，每次 update 的方向都不太一樣，如果這個峽谷非常地窄，有可能一個不小心就跳出去(在多維的情況下，各種方向都可能可以出去)
- 大的 batch 因為只有一個loss fnction就只是順著這條線 update，它就很有可能走到一個比較小的峽谷裡面出不去

### 總結：Batch Size 是一個需要調整的參數，它會影響訓練速度與優化效果

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/c94e4e74-ebba-4b2d-8a60-bb41ebb468e8)
>Batch size大的優點是時間快

>Batch size小的優點是optimization比較好且test data準確也比較好


### 2.2.3 魚與熊掌兼得：結合大、小 Batch 的優點
有可能，這些文章在講

- [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962)
- [Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes](https://arxiv.org/abs/1711.04325)
- [Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well](https://arxiv.org/abs/2001.02312)
- [Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888)
- [Accurate, large minibatch sgd: Training imagenet in 1 hour](https://arxiv.org/abs/1706.02677)

## 2.3 Momentum
另一個可能可以解決saddle point的問題
![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/a4f6e5fb-8647-4e28-83c4-6fb8afee2e41)
>一般球在滾動時在滾動會有動能，不會只單純在凹陷處停止，是有可能再繼續依照動能翻越過去的
### 2.3.1 Vanilla Gradient Descent（一般的梯度下降）

只考慮梯度的方向，往反方向移動

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/229dae21-8c56-4380-94bc-0cc9f8054966)



### 2.3.2 Gradient Descent + Momentum（考慮動量）

綜合梯度 + 前一步的方向

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/40929bf1-52df-48ff-ad47-2814e191f555)
>下一次的方向 = gradient的方向 +上一次移動的方向



所謂的 momentum 就是 update 的方向不是只考慮現在的 gradient，而是**會跟過去所有 gradient 有關**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/0edead17-0b89-44e8-a6d6-d007654a9222)


## 2.4 總結

- critical points 梯度為 0
- saddle point 和 local minima 都屬於 critical point
    - 可由 Hessian matrix 區分
    - local minima 比較少遇到
    - 可藉由沿 Hessian 矩陣的特徵向量方向離開 saddle point
- 較小的 batch size 和 momentum 可幫助離開 critical points

# 第六集
# 3. Adaptive Learning Rate

**現象 1：**
training stuck ≠ small gradient。loss 不再下降時，未必說明此時到達 critical point，梯度可能還很大。實際上走到 critical point 是一件很**困難的事，**有可能只是在**山谷的谷壁間來回走**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/2dabc774-4a21-45e1-a553-3272d6e25b66)


**現象 2：**
如果使用**固定的學習率，即使是在凸面體的優化**，都會讓優化的過程非常困難

- 較大的學習率：loss 在山谷的谷壁間震蕩而不會下降
- 較小的學習率：梯度較小時幾乎難以移動(坡度很平緩根本不太會走，因為步伐超級小)

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/2703356a-de72-49f2-b71d-17230292911f)
>看兩個參數的大小，因為y軸的大小是個位數的大小，但當變成x軸，變成百位數，在同樣的learning rate下就一樣變動個位數的距離，所以很慢


## 3.1 客制化梯度
![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/68407d81-5e35-4655-8814-8e2fa518b907)
>下標的i是指第i個參數
>上標的t是指這個參數更新t次

>這個圖可以看到有w1和w2兩個參數分別是x軸和y軸，而發現x軸再移動的時候那個Loss是變動的比較平緩的，這時候應該要learning rate大一點，而y軸是變動
非常大的，應該要learning rate小一點

>所以不同參數應該是要不同的learning rate的

不同的參數（大小）需要不同的學習率，**新增參數** $\sigma^t_i$(意思為第i個參數的第t次更新的值)

所以根據參數此時的實際情況，調整 $\sigma^t_i$ 的大小，就能客製化每個不同參數的learning rate



### 方法一：**Adagrad**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/ba078b0a-b9b0-4e74-8365-f04f68b84d1c)
>求取 $\sigma$ 的方式：
>**考慮之前所有的梯度大小**
>當前的gradient很大，帶入公式，當前的$\sigma^t_i$就會比較大一點，因為$\sigma^t_i$在分母，所以learning rate就會變小
>
>**Root Mean Square（RMS），**對本次及之前計算出的所有梯度求均方根

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/0f9c02fd-be6e-43b0-aeb1-809cfb7dbb9e)

>考慮不同的參數的loss function
>
>上面的**坡度比較小** ⇒ gradient 較小 ⇒ σ 較小 ⇒ learning rate 大 ⇒ **update 參數的量較大**
>
>下面的**坡度比較大** ⇒ gradient 較大 ⇒ σ 較大 ⇒ learning rate 小 ⇒ **update 參數的量較小**

**缺點：**

不能馬上考慮梯度的變化情況
![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/64246c06-4de6-41a9-bbc4-7e2c5d8f3e9a)
>下面變化變小，需要learning rate變大，但過去的gradient很小，需要很久才會讓learning rate變大


### 方法二：RMSProp
用來解決方法一不能即時調整變動的情況

調整當前梯度與過去梯度的重要性
添加參數 $\alpha$，越大說明過去的梯度訊息**更重要**

- **α 設很小趨近於 0**，代表過去的 gradient 較**不重要**，現在的 $g_i$ 比較**重要**
- **α 設很大趨近於 1**，代表過去的 gradient 較**重要**，現在的 $g_i$ 比較**不重要**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/4bc74496-f04b-4463-a95a-d6fc16ca59e5)
>這裡公式裡面改$\sigma^t_i$是因為$\sigma^t_i$才等於過去所有的gradient設參數就可以調整過去所有和這次的權重
>![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/01fc5ffc-61e7-4456-b71c-6e407e417b61)


### 方法三：Adam = RMSProp + Momentum（最常用的策略）

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/4ff8fa1c-0f2a-465b-9c9e-dfb0e8ef4f59)


使用 PyTorch 中預設的參數就能夠得到比較好的結果
>沒細講，pytorch能直接用，作業裡面Adam就是了

## 3.2 Learning Rate Scheduling

**讓 learning rate 與訓練時間有關**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/a13f29d2-0d7b-463f-9f60-8176a112e7ae)


問題**：**

- 與右上圖相比，右下圖使用 **Adagram** 方法可以繼續往目標前進
- 紅色方框中累積了很小的 gradient，導致累積了很小的 σ，有突然暴增的情況(是因為原本都是很大的gradient做平均，之後一直有小的gradient進來，因為是取全部到目前gradient的平均，所以會越來越小)

**解決：**

**新增 $η^t$，讓它與時間有關**


1.使用Learning Rate Decay：

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/7ebd05aa-5fc7-4757-b89e-4d6c1208dd9f)
隨著時間的不斷地前進，隨著參數不斷的 update，$η^t$ 越來越小(learning rate會隨時間變小，因為朝終點越來越近了)

2.Warm Up：

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/81aa37c9-62a3-47a0-ae3c-cf29872a78f2)
讓 learning rate 先變大後再變小 



**解釋：**

$\sigma$ 指示某一個方向它到底有多陡/多平滑(依據坡度去調整learning rate)，這個統計的結果，要看得夠多筆數據以後才精準，所以一開始我們的統計是不精準的。一開始 learning rate 比較小，是讓它探索收集一些有關 error surface 的情報，在這一階段使用較小的 learning rate，限制參數不會走的離初始的地方太遠；先探索，等到 $\sigma$ 統計得比較精準以後再讓 learning rate 慢慢爬升，就再利用$\sigma$調整

補充：[RAdam](https://arxiv.org/abs/1908.03265) 有更詳細的解釋 Warm Up

## 3.3 總結

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/301bd8b7-8c0e-4e98-9035-f31d464dd782)


- 使用動量，考慮過去的梯度“大小”與“方向”>>調整所要朝向的方向
- 引入 $\sigma$，考慮過去梯度的**大小**（RMS）>>調整learning rate
- 使用 learning rate schedule>>使$\sigma$不會累積到一個程度後learning rate突然爆開，因隨時間離終點越近，leaning rate調整越小

 

# 4. Batch Normalization
這邊在講特徵正規化標準化的意義!!!!!

**現象：**

不同的參數發生變化，引起損失函數變化的程度不同，原因是因**受不同維度輸入值的差異的影響**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/15703579-0e5d-481d-8c2b-44cfc770fe91)


![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/e51d64aa-8ce9-4e37-9548-6660922ef958)
>為w1x1+w2x2+b=y
>
>x軸為w1,y軸為w2


**說明(為甚麼會有這種不同loss變化的情形)：**

- 因 $x_1$ 是乘上 $w_1$， 當 $x_1$ 的值很小時，參數 $w_1$ 有變化時，使對 $y$ 的影響很小，從而對 loss 的影響比較小
- 因 $x_2$ 是乘上 $w_2$， 當 $x_2$ 的值很大時，參數 $w_2$ 有變化時，使對 $y$ 的影響很大，從而對 loss 的影響比較大
>因為是只能調整w1和w2的值，而loss塗上也是w1和w2(比例相同)，w1和w2是x1和x2的乘數，也就是說調整w1和w2，會受到原本的被乘數的大小影響到變動的值有多大，所以x1和x2的影響很有差。

**結論：**

不同維度的輸入值 **scale 差距可能很大**，就會產生在不同方向上，斜率、坡度非常不同的 error surface

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/d1d7c67b-b95d-4473-afac-bf12587f5069)


## 4.1 **Feature Normalization**（歸一化）

### 4.1.1 一種 **Normalization** 方法

對不同 feature 向量的同一維度進行**標準化（Standardization）**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/c1132e32-24ba-4829-bbd8-49f18714f3e3)
>各x維不同的資料，資料裡有很多不同的特徵，標準化就是取同個維度的特徵，把(各資料-全部平均)/標準差


$$
\tilde{x^r_i} = \frac{x^r_i-m_i}{\sigma_i}
$$

- 做完後，這個 dimension 上面的數值平均為 0，variance 為 1，所以**這一列數值的分布會都在 0 上下**
- 對每一個 dimension 都做一樣的 normalization，就會發現所有 feature 不同 dimension 的數值都在 0 上下，就可以**製造一個比較好的 error surface**



### 4.1.2 每一層都需要一次 Normalization

$\tilde{x}$ (已經標準化的特徵)經過 $W_1$ 矩陣後，$a和z$ 數值的分布各維度仍然有很大的差異，要 train 第二層的參數 $W_2$ 也會有困難，因此**需要對 $a$ 或者 $z$ 再進行 normalization**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/4aedd06d-3c0e-4402-bcaa-2402d0df58cf)

此相當於這張圖

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/7f05f42f-50c3-4208-b906-6f69eeabd9d3)

---------------------

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/df5295c7-55c1-4417-af70-03b25eaebf6e)

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/44329f0c-cfc0-421a-860f-d3fd4a2a092c)




一般而言，**normalization 步驟要放在 activation function 之前或之後都是可以的**

如果**選擇的是 sigmoid**，比較**推薦對 $z$ 做 feature normalization**。因為 Sigmoid 在 0 附近斜率比較大，所以如果對 $z$ 做 feature normalization，把所有的值都挪到 0 附近，算 gradient 時，出來的值會比較大

**具體步驟（對 $z$ 做 Feature Normalization）：**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/e58394ef-5744-46cd-ad54-250cbd1a8114)


**1. 對向量的對應 element 做求平均、標準差的運算，求得向量 $\mu,\sigma$**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/2525226e-5ef0-4566-a041-6fef2d17c98a)
**2. 對每個向量 $z$ 利用 $\mu,\sigma$ 對對應 element 進行歸一化，得到** $\tilde{z}$

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/73d2a461-61f0-4468-b4e3-03b76bb67b62)
**3. 繼續後續的步驟**

### 4.1.3 注意：對一批 $z$ 數據進行歸一化

模型變為需**一次處理一批 features 的模型，數據之間相互關聯**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/9db534df-b73d-4703-8891-502e9c2f773e)


- 這μ 跟 $\sigma$ 都是根據 $z^1$, $z^2$, $z^3$ 算出來的，有做 feature normalization，當改變 $z^1$ 的值時，μ 跟 $\sigma$ 也會跟著改變，當 μ 跟 $\sigma$ 改變後，$z^2$ 的值、$a^2$ 的值、$z^3$ 的值、$a^3$ 的值也會跟著改變
- 之前每一個 $\tilde{x}$ 都是獨立分開處理的，但當做 feature normalization 後，這三個 feature 變得彼此關聯
- 之前的 network 都吃一個 feature 得到一個 output，現在有一個比較大的 network，這個 network 是吃一堆 features 用這堆 features 在這個 network 裡面，要算出 μ 跟 $\sigma$，然後產生一堆 output
>這邊的意思應該是
>![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/28250d51-5c05-4262-a7b4-9244143b670c)
>
>![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/80935199-ed8c-4db7-829e-bfd0b8f35926)
>一個network是一個W的行和x向量




### 4.1.4 Batch Normalization

Batch Normalization 適用於 batch size 比較大(因為算平均和標準差數量太少沒什麼意義)。一個 batch 內的 data 可以認為足以表示整個 corpus 的分布(因為此batch的size就夠大足以表示全部)，本來要對整個 corpus 做 feature normalization 這件事情，改成**只在一個 batch 中做 feature normalization 作為近似**

所以實際上做 normalization 時，只考慮有限數量的數據，即**考慮一個 batch 內的數據，近似整個數據集**

**還原：**

引入向量 $\gamma,\beta$，將原本被歸一化到 $μ=0,\sigma=1$ 的各維度資料恢復到某一分布

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/94db7c9c-850b-4925-99b5-2bc90c82d7ca)


做 normalization 以後 $\tilde{z}$ 平均就一定是 0，可以視作是給 network 一些限制，但這個限制可能會帶來負面的影響，因而增加 $\gamma,\beta$ 參數

訓練時**初始將 $\gamma$ 設為全為 1 的向量，$\beta$ 設為全為 0 的向量**。在一開始訓練的時，讓每一個 dimension 的分布比較接近。**訓練夠長的一段時間**，已經走**到一個比較好的 error surface**，那**再把 $\gamma,\beta$ 慢慢地加進去**

### 4.1.5 Testing 時會遇到的問題

testing（inference）時**沒有 Batch 進行歸一化** ⇒ **Training 時會紀錄 $\bar{\mu}$ 及 $\bar{\sigma}$**

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/97993053-5521-4898-b4db-df1b0c2513de)
（p 是超參數）

在 testing 階段一筆資料進來就要產生結果，沒有一個“batch”的數據可以進行歸一化，所以在 PyTorch 中，training 時就會把每一個 batch 計算出來的 $\mu$ 跟 $\sigma$，拿出來算 **moving average**

### 4.1.6 結果比較

紅色虛線（有做 Batch Normalization）收斂的速度顯然比黑色虛線（沒有做 Batch Normalization）要快很多
>比較快的時間達到準確 

![image](https://github.com/ShangZheTsai/NTU-Deep-Learning/assets/107501046/52cad0cc-9ebd-4ce5-b604-9dff2466a5b9)
橫軸：訓練過程，縱軸：準確率（Validation set）

**為什麼 Batch Normalization 會比較好？**

在 [*How Does Batch Normalization Help Optimization*](https://arxiv.org/abs/1805.11604) 論文中，作者從實驗上，也從理論上，至少支持了 batch normalization 可以改變 error surface，讓 error surface 比較不崎嶇這個觀點

### 4.1.7 其他 Normalization 方法

- [Batch Renormalization](https://arxiv.org/abs/1702.03275)
- [Layer Normalization](https://arxiv.org/abs/1607.06450)
- [Instance Normalization](https://arxiv.org/abs/1607.08022)
- [Group Normalization](https://arxiv.org/abs/1803.08494)
- [Weight Normalization](https://arxiv.org/abs/1602.07868)
- [Spectrum Normalization](https://arxiv.org/abs/1705.10941)
